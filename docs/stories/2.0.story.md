# Story 2.0: LLM Provider Integration & Agent Foundation

## Status

Draft

## Story

**As a** system architect,
**I want** LLM providers configured with LangGraph agent orchestration foundation,
**so that** AI agents can process conversations and make intelligent routing decisions for all Epic 2 functionality.

## Acceptance Criteria

**LLM Provider Configuration:**
1. LLM API key configuration system supporting multiple providers (OpenAI, Anthropic, etc.)
2. Environment variable management for LLM credentials with secure storage
3. LLM client initialization with error handling and fallback strategies
4. Provider selection configuration allowing runtime switching between LLM providers

**LangGraph Foundation:**
5. Core LangGraph workflow setup with GlobalState model for shared agent context
6. LLM integration layer connecting LangGraph agents to configured LLM providers
7. Basic conversation state management with SQLite checkpointing for persistence
8. Error handling and graceful degradation for LLM API failures

**Orchestrator Agent Implementation:**
9. Orchestrator Agent with LLM-powered intent recognition and agent routing
10. Conversation context analysis to determine appropriate specialized agent
11. Direct response capability for simple queries that don't require agent routing
12. Agent handoff mechanism with context preservation between agent switches

**Technical Foundation:**
13. GlobalState pattern implementation for cross-agent data sharing
14. MCP client integration for centralized tool access across all agents
15. Conversation API endpoints for LangGraph workflow invocation
16. Integration with existing JWT authentication for user context in agent operations

## Tasks / Subtasks

### LLM Provider Configuration
- [ ] Implement LLM configuration system (AC: 1, 2)
  - [ ] Add LLM provider settings to backend/app/core/config.py
  - [ ] Create environment variables for OPENAI_API_KEY, ANTHROPIC_API_KEY
  - [ ] Add provider selection configuration (DEFAULT_LLM_PROVIDER)
  - [ ] Implement secure credential validation on startup
- [ ] Create LLM client service (AC: 3, 4)
  - [ ] Implement LLMService in backend/app/services/llm_service.py
  - [ ] Add multi-provider client initialization (OpenAI, Anthropic)
  - [ ] Implement provider fallback and error handling
  - [ ] Add LLM health check and connectivity validation

### LangGraph Agent Foundation
- [ ] Setup core LangGraph infrastructure (AC: 5, 7)
  - [ ] Create backend/app/ai/langgraph_config.py for workflow configuration
  - [ ] Implement GlobalState model for shared agent context
  - [ ] Configure SQLite checkpointer for conversation persistence
  - [ ] Add LangGraph workflow initialization and lifecycle management
- [ ] Integrate LLM service with LangGraph (AC: 6, 8)
  - [ ] Connect LLMService to LangGraph agent nodes
  - [ ] Implement LLM error handling within agent workflows
  - [ ] Add retry logic and graceful degradation for API failures
  - [ ] Create LLM response validation and sanitization

### Orchestrator Agent Implementation  
- [ ] Create Orchestrator Agent with intelligent routing (AC: 9, 10, 11)
  - [ ] Implement OrchestratorAgent in backend/app/ai/orchestrator_agent.py
  - [ ] Add LLM-powered intent analysis from user messages
  - [ ] Create agent routing logic with confidence scoring
  - [ ] Implement direct response capability for simple queries
- [ ] Add agent coordination and handoff (AC: 12)
  - [ ] Create agent handoff mechanism with context preservation
  - [ ] Implement conversation flow management between agents
  - [ ] Add agent state tracking and coordination

### Integration & API Layer
- [ ] Implement foundational conversation endpoints (AC: 13, 15)
  - [ ] Add POST /api/conversation/send with LangGraph orchestration
  - [ ] Create conversation session management with GlobalState
  - [ ] Implement streaming response capability for LLM interactions
- [ ] Integrate with existing systems (AC: 14, 16)
  - [ ] Connect MCP client to GlobalState for cross-agent tool access
  - [ ] Integrate JWT authentication for user context in agents
  - [ ] Add user session management and conversation threading

### Testing & Validation
- [ ] Add comprehensive testing coverage
  - [ ] Unit tests for LLMService with mocked provider clients
  - [ ] Unit tests for OrchestratorAgent routing logic
  - [ ] Integration tests for LangGraph workflow execution
  - [ ] API tests for conversation endpoints with LLM integration
- [ ] Implement monitoring and observability
  - [ ] Add LLM API usage metrics and cost tracking
  - [ ] Implement conversation flow logging for debugging
  - [ ] Create health checks for LLM provider availability

## Dev Notes

### Previous Story Context
From Story 1.6 completion:
- MCP server integrated with FastMCP v2.12.0 providing Plaid tools
- JWT authentication system ready for user context
- FastAPI backend with /mcp endpoint for tool access
- PlaidService abstraction layer available for agents

### LLM Provider Architecture
**Multi-Provider LLM Service** [Source: architecture/tech-stack.md]:
```python
class LLMService:
    def __init__(self):
        self.providers = {
            "openai": OpenAI(api_key=settings.OPENAI_API_KEY),
            "anthropic": Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        }
        self.default_provider = settings.DEFAULT_LLM_PROVIDER
    
    async def generate_response(self, messages: List[Dict], provider: str = None):
        provider = provider or self.default_provider
        # Provider-specific implementation with fallback
```

**Environment Configuration**:
```bash
# LLM Provider Configuration
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
DEFAULT_LLM_PROVIDER=openai  # or anthropic
LLM_MAX_TOKENS=4096
LLM_TEMPERATURE=0.7
```

### LangGraph Agent Architecture
**GlobalState Model** [Source: architecture/components.md#global-state-management]:
```python
class GlobalState(BaseModel):
    # User Context
    user_id: str
    session_id: str
    conversation_history: List[Dict[str, Any]]
    
    # Current Processing
    user_message: str
    current_intent: Optional[str] = None
    active_agent: Optional[str] = None
    
    # Shared Data
    user_profile: Optional[Dict[str, Any]] = None
    connected_accounts: List[Dict[str, Any]] = []
    
    # Agent Coordination
    agent_handoff_context: Optional[Dict[str, Any]] = None
    mcp_tool_results: Dict[str, Any] = {}
```

**Orchestrator Agent Routing Logic**:
```python
async def analyze_intent(state: GlobalState) -> Dict[str, Any]:
    # LLM-powered intent analysis
    response = await llm.ainvoke([
        SystemMessage("Analyze user intent and route to appropriate agent..."),
        HumanMessage(state.user_message)
    ])
    
    # Extract routing decision with confidence
    routing = parse_routing_response(response)
    return {
        "target_agent": routing.agent,
        "confidence": routing.confidence,
        "reasoning": routing.reasoning
    }
```

### API Specifications
**Conversation Endpoints** [Source: architecture/components.md#ai-conversation-service]:
- POST /api/conversation/send - Main conversation endpoint with LangGraph orchestration
- Streaming response support for real-time LLM interactions
- Session management with conversation threading
- Integration with JWT authentication for user context

### File Locations
**Backend Files** [Source: architecture/unified-project-structure.md]:
- LLM service: `backend/app/services/llm_service.py`
- LangGraph config: `backend/app/ai/langgraph_config.py`
- Orchestrator agent: `backend/app/ai/orchestrator_agent.py`
- Conversation router: `backend/app/routers/conversation.py`
- Core configuration: `backend/app/core/config.py`

### Technical Constraints
- **LLM Providers**: OpenAI GPT-4 and/or Anthropic Claude with fallback capability [Source: architecture/tech-stack.md]
- **State Management**: LangGraph SQLite checkpointing for conversation persistence [Source: architecture/tech-stack.md]
- **Authentication**: JWT integration for user context in agent operations [Source: architecture/tech-stack.md]
- **Tool Access**: MCP client integration for centralized tool access [Source: architecture/tech-stack.md]

### Security Requirements
- LLM API keys stored as environment variables, never hardcoded [Source: architecture/security-and-performance.md#backend-security]
- JWT token validation for all conversation endpoints [Source: architecture/security-and-performance.md#authentication-security]
- LLM request/response logging with PII sanitization [Source: architecture/security-and-performance.md#financial-data-security]
- Rate limiting and usage monitoring for LLM API calls

### Testing

**Testing Standards from Architecture** [Source: architecture/testing-strategy.md]:

**Test Framework**: pytest with pytest-asyncio for backend async tests
**Test Location**: `backend/tests/` directory  
**Test File Naming**: `test_*.py` pattern

**Specific Testing Requirements for This Story**:
1. Unit tests for LLMService with mocked API responses from multiple providers
2. Unit tests for OrchestorAgent routing logic with various user inputs
3. Integration tests for LangGraph workflow execution with GlobalState
4. API tests for conversation endpoints with streaming response validation
5. Error handling tests for LLM provider failures and fallback scenarios

**Test Organization**:
```text
backend/tests/
├── test_llm_service.py           # LLM provider integration
├── test_orchestrator_agent.py    # Agent routing logic  
├── test_langgraph_workflow.py    # LangGraph execution
└── integration/
    ├── test_conversation_flow.py # End-to-end conversation
    └── test_agent_coordination.py # Agent handoff scenarios
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-04 | 1.0 | Initial story creation for LLM foundation before Epic 2 agents | SM Agent |

## Dev Agent Record

### Agent Model Used
*To be populated during implementation*

### Debug Log References  
*To be populated during implementation*

### Completion Notes List
*To be populated during implementation*

### File List
*To be populated during implementation*

## QA Results
*To be populated after implementation*