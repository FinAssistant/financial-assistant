# Story 2.3: Spending Agent - Transaction Analysis and Insights

## Status
In Progress

## Story
**As a** user who wants to understand and optimize my spending habits,
**I want** an AI agent that provides comprehensive transaction analysis, spending insights, and personalized optimization recommendations,
**so that** I can make informed financial decisions and improve my financial health through conversational interaction.

**Architecture References:**
- Dual storage patterns: `docs/architecture/dual-storage-architecture.md`
- Component integration: `docs/architecture/components.md`
- Conversation patterns: `docs/stories/1.4.story.md` (AI Conversation Interface foundation)

## Acceptance Criteria

### Technical Foundation
1. Spending Agent implementation as LangGraph subgraph integrated with existing conversation system
2. Intelligent transaction fetching via MCP server Plaid tools with incremental updates
3. AI-powered transaction categorization integrated into agent workflow (user feedback learning planned for future iteration)
4. Transaction accuracy features: re-categorization, split transactions, pending transaction management within conversational interface

### Analysis & Intelligence
5. Comprehensive spending pattern recognition (recurring expenses, seasonal trends, anomalies) delivered conversationally
6. Cash flow analysis with trend detection and forecasting capabilities presented through AI conversation
7. Spending personality profiling delivered through natural language explanations
8. Behavioral insight generation (spending triggers, timing patterns, stress spending) via conversational analysis
9. Risk tolerance assessment and financial motivation profiling through AI conversation

### Optimization Engine
10. Smart expense reduction recommendations with personality-tailored strategies delivered conversationally
11. Subscription and recurring payment optimization (duplicates, unused services, consolidation) via AI insights
12. Cost optimization opportunities with savings calculations based on spending patterns
13. Residual cash optimization with surplus allocation recommendations through conversation

### Budget & Planning
14. Conversational budget guidance with spending awareness and category insights
15. Intelligent budget recommendations delivered through AI conversation based on spending history and personality
16. Budget alerts and variance insights delivered conversationally with adjustment suggestions
17. Goal alignment analysis comparing spending habits to stated objectives through AI conversation

### User Experience & Interface
18. Conversational transaction queries with natural language processing
19. Personality-aware communication with adaptive tone and messaging based on user insights stored in Graphiti

### Technical Implementation Details
20. LangGraph subgraph implementation following established agent patterns from Stories 1.1-1.4
21. Dual storage integration: SQLite for structured data (user demographics + canonical transaction storage), Graphiti for conversational insights and derived analysis results
22. MCP tool usage: Plaid tools for external data, SQLite for canonical transaction storage, Graphiti tools for insight storage, agent-integrated analysis logic
23. Integration with existing conversation API (`/conversation/send`) rather than separate analysis endpoints
24. Reference architecture: Follow patterns documented in `dual-storage-architecture.md` and `components.md`

### Testing & Validation
25. Unit tests for spending analysis algorithms integrated within agent nodes
26. Integration tests for LangGraph subgraph workflow and state management
27. Conversation quality testing for AI analysis delivery and user comprehension
28. Dual storage testing: SQLite context retrieval and Graphiti insight storage validation

## Tasks / Subtasks

- [x] **LangGraph Spending Agent Subgraph** (AC: 1, 20, 21)
  - [x] Create SpendingAgent class following LangGraph patterns from Stories 1.1-1.4
  - [x] Implement agent initialization node with SQLite context retrieval
  - [x] Implement LLM-powered agent workflow routing with intelligent intent detection
  - [x] Implement LLM-powered conversational response generation

- [ ] **Agent-Integrated Analysis Logic** (AC: 2, 3, 4, 5, 6, 7, 8, 9)
  - [x] Integrate transaction fetching via MCP Plaid tools within agent workflow
  - [x] Implement AI-powered transaction categorization within agent processing
  - [ ] Integrate spending pattern analysis (recurring expenses, seasonal trends, anomalies)
  - [ ] Integrate cash flow analysis with trend detection
  - [ ] Implement spending personality profiling algorithm within agent
  - [ ] Integrate behavioral insight generation logic
  - [ ] Implement risk tolerance assessment within agent workflow
  - [ ] Add transaction re-categorization, split transactions, pending transaction management

- [x] **Dual Storage Integration** (AC: 21, 22, 24)
  - [x] Integrate SQLite service for reading user demographics and personal context (COMPLETE)
  - [x] Implement SQLite canonical transaction storage with schema and models
  - [x] Integrate transaction storage service in SpendingAgent workflow
  - [x] Integrate Graphiti via MCP tools for storing spending insights and analysis results
  - [x] Implement data flow patterns: SQLite (canonical transactions + demographics) → Analysis → Graphiti (store insights)
  - [x] Add Graphiti integration using `get_graphiti_client()` to get instance of Graphiti MCP client and `add_episode` and `search` for episode storage and context search
  - [x] Unified SQLiteUserStorage for both PersonalContext and Transactions
  - [x] Database consolidation removing redundant DAO classes
  - [x] Tag-based search matching with normalized_month field for consistency

- [ ] **Conversational Optimization Engine** (AC: 10, 11, 12, 13)
  - [ ] Implement personality-based optimization recommendations within agent responses
  - [ ] Integrate subscription optimization algorithm with conversational delivery
  - [ ] Implement cost optimization based on spending patterns for conversational presentation
  - [ ] Integrate surplus allocation recommendation engine with AI conversation

- [ ] **Conversational Budget & Planning** (AC: 14, 15, 16, 17)
  - [ ] Implement conversational budget guidance delivery through agent responses
  - [ ] Integrate budget recommendations for conversational delivery format
  - [ ] Transform budget alerts into conversational insights with adjustment suggestions
  - [ ] Implement goal alignment analysis for conversational presentation

- [ ] **Conversational Interface Integration** (AC: 18, 19, 23)
  - [x] Replace dummy spending agent in orchestrator with real SpendingAgent subgraph
  - [ ] Implement personality-aware response generation using Graphiti context
  - [x] Add natural language transaction query processing within agent (LLM-driven query parser)
  - [x] Implement intelligent result formatting (summaries, breakdowns, individual transactions)
  - [x] Add time range context to system prompts (start_date, end_date, days_back)
  - [ ] Implement adaptive communication tone based on stored user insights

- [x] **Testing & Validation** (AC: 25, 26, 27, 28)
  - [x] Create unit tests for agent-integrated analysis algorithms
  - [x] Implement integration tests for LangGraph subgraph workflow and state management
  - [x] Add conversation quality testing for AI analysis delivery and user comprehension
  - [x] Create dual storage testing: SQLite context retrieval and Graphiti insight storage validation
  - [x] Test MCP tool integration for Plaid data fetching and Graphiti storage

## Dev Notes

### Architecture Overview
This story implements a conversational Spending Agent as a LangGraph subgraph that integrates with the existing conversation system established in Stories 1.1-1.4. All analysis and insights are delivered through natural conversation rather than separate APIs or complex UI components.

### Key Integration Points
- **Conversation System**: Integrates with `/conversation/send` endpoint from Story 1.4
- **Authentication**: Uses existing auth system from Stories 1.1-1.2
- **Data Storage**: Follows dual storage pattern (SQLite for demographics + transactions, Graphiti for insights)
- **External Data**: Uses MCP Plaid tools for transaction fetching

### Technology Stack
- **AI Framework**: LangGraph subgraph architecture
- **External Integration**: MCP tools for Plaid and Graphiti access
- **Storage**: SQLite (demographics + transaction ledger) + Graphiti (conversational insights)
- **Transaction Storage**: SQLite canonical ledger with Graphiti insight generation pipeline
- **Delivery**: Conversational interface via existing chat system

### Testing Strategy
Focus on conversation quality and agent workflow testing rather than API endpoint testing. Validate that financial insights are delivered clearly and comprehensibly through AI conversation.

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-01-25 | 2.0 | Complete rewrite for conversational architecture alignment | SM Agent |
| 2025-09-02 | 1.0 | Initial story creation | SM Agent |

## Dev Agent Record

### Agent Model Used
- Claude Sonnet 4 (claude-sonnet-4-20250514)

### Debug Log References  
*This section will be populated by the development agent during implementation*

### Completion Notes List

#### ✅ COMPLETED: Basic SpendingAgent Foundation (Aug 2025)

**What Was Actually Implemented:**
- ✅ **SpendingAgent LangGraph Subgraph**: Basic 3-node architecture with conditional routing to 5 specialized intent nodes
- ✅ **SQLite Integration**: Real UserContextDAO integration for retrieving user demographics and financial context 
- ✅ **Intent-Based Routing**: Keyword-based intent detection routing to specialized nodes (spending_analysis, budget_planning, optimization, transaction_query, general_spending)
- ✅ **Professional Response Generation**: Static professional responses for each intent type
- ✅ **Comprehensive Testing**: 16 tests covering initialization, routing, specialized nodes, and error handling (all passing)

**Architecture Implemented:**
```
START → initialize (real DAO) → route_intent → [specialized_node] → END
```

**Key Limitations - What's NOT Implemented:**
- ❌ **No Real Analysis**: All responses are static text, no actual transaction analysis
- ❌ **No MCP Tool Integration**: No Plaid data fetching, no Graphiti storage
- ❌ **No AI-Powered Features**: No LLM calls, just keyword-based intent detection
- ❌ **No Financial Logic**: No spending patterns, cash flow analysis, optimization algorithms
- ❌ **No API Integration**: Not integrated with `/conversation/send` endpoint
- ❌ **Mock Personality**: Simple professional tone, no real personality adaptation

**Status**: This completes only the **basic infrastructure foundation** for the SpendingAgent. The actual financial analysis, MCP integrations, and AI-powered features remain unimplemented.

#### ✅ COMPLETED: LLM Integration Architecture (September 2025)

**What Was Additionally Implemented:**
- ✅ **LLM-Powered Intent Detection**: Replaced keyword-based intent detection with real LLM calls using structured prompts
- ✅ **Multi-LLM Provider Support**: Extended LLM service to support OpenAI, Anthropic Claude, and Google Gemini
- ✅ **Real LLM Integration**: SpendingAgent now uses self.llm for intelligent intent classification 
- ✅ **Provider-Specific Configuration**: Pydantic models with provider-specific settings (OpenAI extra="forbid")
- ✅ **Comprehensive LLM Testing**: Real LLM integration tests for all three providers with proper environment-based skipping
- ✅ **API Compatibility Fixes**: Resolved Gemini API message format requirements
- ✅ **Fallback Intent Detection**: Graceful degradation when LLM is unavailable
- ✅ **Configuration System Updates**: Environment variable support for all three LLM providers

**Architecture Enhanced:**
```
START → initialize (real DAO) → route_intent (LLM-powered) → [specialized_node] → END
```

**Key Progress - What's NOW Implemented:**
- ✅ **Real LLM Calls**: Intent detection uses actual LLM providers instead of keyword matching
- ✅ **Multi-Provider Architecture**: Supports OpenAI GPT-4o, Anthropic Claude Sonnet, Google Gemini Flash
- ✅ **Intelligent Routing**: Context-aware intent detection using user demographics and message content
- ✅ **Provider Fallbacks**: Graceful handling of API failures and missing credentials
- ✅ **Real API Testing**: Integration tests that validate all three LLM providers

**Key Limitations - What's Still NOT Implemented:**
- ❌ **No LLM Response Generation**: Intent nodes still use static responses instead of LLM-generated content
- ❌ **No Real Analysis**: No actual transaction analysis or financial calculations
- ❌ **No MCP Tool Integration**: No Plaid data fetching, no Graphiti storage  
- ❌ **No Financial Logic**: No spending patterns, cash flow analysis, optimization algorithms
- ❌ **No API Integration**: Not integrated with `/conversation/send` endpoint

**Status**: This completes the **LLM integration architecture** for intelligent intent routing. The next phase requires implementing LLM-powered response generation and integrating real financial data analysis.

#### ✅ COMPLETED: LLM-Powered Response Generation (September 2025)

**What Was Additionally Implemented:**
- ✅ **Complete LLM Response Generation**: All 5 intent nodes now use LLM-powered conversational responses
- ✅ **Reusable Utility Functions**: Created `_build_user_context_string()` for consistent context formatting across nodes
- ✅ **Personalized Financial Analysis**: Spending analysis node provides detailed insights with mock financial data
- ✅ **Budget Planning Guidance**: AI-generated budget recommendations based on user context and financial situation  
- ✅ **Optimization Recommendations**: Cost-saving suggestions with specific opportunities and potential savings
- ✅ **Intelligent Transaction Queries**: LLM-enhanced responses when data exists, functional routing when fetching
- ✅ **Personalized Welcome Experience**: General spending node provides context-aware introductions
- ✅ **Consistent Error Handling**: All nodes provide uniform "service temporarily unavailable" messages
- ✅ **Comprehensive Testing**: All 33 tests passing with proper LLM mocking and state handling

**Architecture Enhanced:**
```
START → initialize (real DAO) → route_intent (LLM-powered) → [LLM-powered response nodes] → END
```

**Response Nodes Now LLM-Powered:**
1. **Spending Analysis**: Personalized financial analysis with spending breakdowns and insights
2. **Budget Planning**: Customized budget guidance with specific recommendations and allocations
3. **Optimization**: Cost reduction recommendations with actionable strategies
4. **Transaction Queries**: Smart LLM enhancement when presenting existing data, functional routing when fetching
5. **General Spending**: Warm, personalized welcome and capability introduction

**Key Technical Achievements:**
- ✅ **User Context Integration**: All nodes use structured user demographics and financial context
- ✅ **Mock Data Integration**: Realistic financial scenarios for demonstration purposes
- ✅ **Error Handling Consistency**: Unified fallback responses across all nodes
- ✅ **LangGraph State Management**: Proper dictionary access patterns for `MessagesState`
- ✅ **Reusable Code Architecture**: Shared utility functions reduce duplication

**Key Limitations - What's Still NOT Implemented:**
- ❌ **No Real Financial Data**: Uses mock data structures for demonstration
- ❌ **No MCP Tool Integration**: No actual Plaid data fetching or Graphiti storage
- ❌ **No Real Analysis Algorithms**: No spending pattern analysis, cash flow analysis, or optimization algorithms
- ❌ **No API Integration**: Not integrated with `/conversation/send` endpoint
- ❌ **No Advanced AI Features**: No transaction categorization, personality profiling, or behavioral insights

**Status**: This completes the **LLM-powered conversational foundation** for the SpendingAgent. All intent nodes now provide intelligent, personalized responses. The next phase requires integrating real financial data and implementing advanced analysis algorithms.

#### ✅ COMPLETED: AI-Powered Transaction Categorization Integration (September 2025)

**What Was Additionally Implemented:**
- ✅ **Complete Transaction Categorization Service**: Full LLM-powered transaction categorization with structured output
- ✅ **SpendingAgent Integration**: Categorization service fully integrated into SpendingAgent with proper initialization
- ✅ **Real Transaction Processing**: Enhanced `_fetch_and_process_node` to apply AI categorization to fetched transactions
- ✅ **Context-Aware Categorization**: User demographics and financial context passed through categorization pipeline
- ✅ **Utility Methods**: `_categorize_and_apply_transactions` and `_apply_categorization_to_transaction` for transaction enhancement
- ✅ **Dynamic Batch Sizing**: Context window-aware batch sizing for different LLM providers (OpenAI, Claude, Gemini)
- ✅ **Comprehensive Error Handling**: Graceful fallback when categorization fails with detailed logging
- ✅ **Code Deduplication**: Refactored LLM error handling into reusable `_invoke_llm_with_fallback` method
- ✅ **Shared Test Fixtures**: Created reusable transaction fixtures to eliminate test duplication
- ✅ **Integration Testing**: Mock and real LLM integration tests for categorization service + SpendingAgent
- ✅ **Multi-Provider Testing**: Real LLM categorization tests for OpenAI, Anthropic Claude, and Google Gemini

**Categorization Features Implemented:**
- ✅ **AI Category Assignment**: Primary categories (Food & Dining, Transportation, Shopping, etc.)
- ✅ **AI Subcategory Assignment**: Specific subcategories (Coffee Shops, Gas Stations, etc.)
- ✅ **Confidence Scoring**: 0.0-1.0 confidence levels for categorization quality assessment
- ✅ **Intelligent Tagging**: Contextual tags for transactions (recurring, subscription, etc.)
- ✅ **Reasoning Explanation**: AI-generated explanations for categorization decisions
- ✅ **User Context Integration**: Demographics and financial context influence categorization
- ✅ **Batch Processing**: Efficient categorization of multiple transactions simultaneously

**Technical Architecture Enhanced:**
```
START → initialize → route_intent → transaction_query → fetch_and_process
                                                      ↓
                                        [Real Transaction Fetch] → [AI Categorization] → [Enhanced Response]
```

**Testing Coverage:**
- ✅ **Unit Tests**: 13 transaction categorization service tests (9 passing, 4 skipped)
- ✅ **Mock Integration**: 3 SpendingAgent + categorization integration tests
- ✅ **Real LLM Integration**: 3 real LLM categorization tests (skipped without API keys)
- ✅ **Cross-Provider Testing**: Tests for OpenAI, Anthropic, and Google LLM providers

**Key Technical Achievements:**
- ✅ **Structured LLM Output**: Pydantic models ensure consistent categorization format
- ✅ **Context Window Management**: Dynamic batch sizing prevents token limit issues
- ✅ **Personalized Analysis**: User context enhances categorization accuracy
- ✅ **Production-Ready**: Comprehensive error handling and fallback mechanisms

**Key Progress - What's NOW Implemented:**
- ✅ **Real AI Transaction Categorization**: Actual LLM-powered categorization integrated into workflow
- ✅ **Transaction Enhancement**: Raw Plaid transactions enhanced with AI insights
- ✅ **User-Specific Context**: Demographics and financial profile influence categorization decisions
- ✅ **Quality Assurance**: Confidence scoring and reasoning for categorization transparency

**Key Limitations - What's Still NOT Implemented:**
- ❌ **No Graphiti Storage**: Categorized transactions not yet stored in Graphiti for memory/retrieval
- ❌ **No Learning Loop**: No user feedback mechanism to improve categorization accuracy
- ❌ **No Advanced Analysis**: No spending patterns, cash flow analysis, or behavioral insights
- ❌ **No API Integration**: Not integrated with `/conversation/send` endpoint
- ❌ **No Recurring Detection**: No automatic categorization of recurring transactions

**Status**: This completes the **AI-powered transaction categorization foundation**. Transactions are now intelligently categorized with AI insights and integrated into the SpendingAgent workflow. The next phase requires Graphiti integration for persistent storage and advanced financial analysis algorithms.

#### ✅ COMPLETED: Graphiti Integration and Storage (September 2025)

**What Was Additionally Implemented:**

- ✅ **Complete Graphiti Integration**: Full `get_graphiti_client()` integration for memory storage and retrieval
- ✅ **Transaction Episode Storage**: Batch storage using `batch_store_transaction_episodes()` with concurrency optimization
- ✅ **Search Integration**: Graphiti search functionality integrated into `_transaction_query_node` for context retrieval
- ✅ **Data Flow Implementation**: Complete SQLite (read demographics) → Analysis → Graphiti (store insights) pipeline
- ✅ **Duplicate Detection**: Built-in duplicate checking during transaction storage to prevent redundant data
- ✅ **Error Handling**: Comprehensive error handling for Graphiti connectivity and storage failures
- ✅ **Performance Optimization**: Concurrent storage processing (concurrency=8) for efficient batch operations
- ✅ **Storage Analytics**: Detailed logging of stored vs duplicate vs error transaction counts
- ✅ **Context Memory**: Transactions stored as episodes with full context for future retrieval and analysis

**Technical Architecture Enhanced:**

```mermaid
START → initialize → route_intent → transaction_query ←→ [Graphiti Search]
                                         ↓
                                   fetch_and_process → [AI Categorization] → [Graphiti Storage]
```

**Graphiti Integration Features:**

- ✅ **Episode Storage**: Categorized transactions stored as contextual episodes
- ✅ **Search Retrieval**: Query-based search for relevant transaction insights
- ✅ **Batch Processing**: Efficient bulk storage with concurrency control
- ✅ **Duplicate Prevention**: Automatic detection and skipping of existing transactions
- ✅ **Connection Management**: Robust connection checking and error recovery
- ✅ **User Isolation**: User-specific storage and retrieval for data privacy

**Key Progress - What's NOW Implemented:**

- ✅ **Complete Dual Storage**: SQLite demographics + Graphiti transaction insights fully functional
- ✅ **Persistent Memory**: Transaction data persisted for future conversational context
- ✅ **Context Retrieval**: Past transaction insights available during new conversations
- ✅ **Storage Resilience**: Graceful degradation when Graphiti is unavailable

**Key Limitations - What's Still NOT Implemented:**

- ❌ **No Advanced Analysis**: No spending patterns, cash flow analysis, or behavioral insights algorithms
- ❌ **No API Integration**: Not integrated with `/conversation/send` endpoint (still uses dummy agent)
- ❌ **No Optimization Engine**: No cost reduction, subscription analysis, or surplus allocation
- ❌ **No Budget Planning**: No conversational budget guidance or recommendations

**Status**: This completes the **core storage and memory foundation** for the SpendingAgent. All transaction data is now persistently stored with AI categorization and available for future retrieval. The next phase requires implementing advanced financial analysis algorithms and orchestrator integration.

**Note**: User feedback mechanism for categorization accuracy improvement has been removed from this iteration and planned for a future release to focus on core functionality first.

#### ✅ INTEGRATION ARCHITECTURE UNDERSTANDING (September 2025)

**Integration Flow Confirmed:**
```
POST /conversation/send → ConversationHandler.process_message() → OrchestratorAgent.invoke_conversation() → [LLM Routing] → SpendingAgent.graph.compile()
```

**Key Integration Points:**
- ✅ **API Integration Architecture**: Confirmed `/conversation/send` API flows through `ConversationHandler → OrchestratorAgent → [Route] → SpendingAgent`
- ✅ **Integration Pattern**: SpendingAgent integration happens by replacing `_dummy_spend_agent` in orchestrator with compiled SpendingAgent subgraph
- ✅ **Response Format Evolution**: API supports multiple message array format for complex agent interactions
- ✅ **State Management**: SpendingAgent uses `GlobalState` for cross-agent data sharing and conversation persistence
- ✅ **Test Infrastructure**: Integration tests updated to handle new multi-message response format

**Ready for Implementation**: All architectural analysis complete. Next step is replacing dummy agent with real SpendingAgent subgraph in orchestrator.

#### ✅ COMPLETED: Transaction Storage & Query System (October 2025)

**What Was Additionally Implemented:**

**Database Consolidation:**
- ✅ **Unified SQLiteUserStorage**: Merged user context and transaction storage into single storage class
- ✅ **Database Cleanup**: Removed redundant `user_context_dao.py` and consolidated all storage operations
- ✅ **Test Migration**: Ported all essential tests from deleted files to unified test suite
- ✅ **Single Storage Instance**: SpendingAgent now uses `self._storage` for both user context and transactions

**Transaction Query System:**
- ✅ **LLM-Driven Query Parser**: Natural language → structured `TransactionQueryIntent` extraction
- ✅ **Safe SQL Execution**: Parameterized SQL templates prevent injection attacks
- ✅ **Query Intent Types**: Support for 9 query types (spending_summary, spending_by_category, search_by_merchant, etc.)
- ✅ **Transaction Query Node**: Dedicated LangGraph node for processing transaction queries
- ✅ **Context-Aware Formatting**: Smart formatter detects and formats 3 data types (summaries, breakdowns, transactions)
- ✅ **Time Range Context**: System prompt includes start_date, end_date, days_back for accurate LLM responses

**Result Formatting Architecture:**
- ✅ **Data Structure Detection**: `_identify_data_structure()` determines format type from actual data
- ✅ **Type-Based Formatting**: `_format_item_by_type()` handles spending summaries, category breakdowns, individual transactions
- ✅ **Query Intent Header**: Results include "Query Type: Spending Summary" for better LLM context
- ✅ **Code Deduplication**: Reduced formatter code by ~70% through unified formatting functions

**Flow Improvements:**
- ✅ **Context-Aware Errors**: Progressive error messages based on retry attempt (1/3, 2/3, 3/3)
- ✅ **Fetch Timeout**: 30-second timeout for Plaid operations with proper async handling
- ✅ **State Flow Tests**: 7 comprehensive tests validating retry logic and completion states

**Technical Architecture Enhanced:**
```
START → initialize → route_intent → transaction_query_node
                                         ↓
                    [Query exists?] → [LLM Parser] → [SQL Executor] → [Result Formatter] → [LLM Response]
                           ↓
                    [No data] → fetch_and_process → [Plaid Fetch] → [AI Categorization] → [SQLite Storage]
```

**Testing Coverage:**
- ✅ **42 SpendingAgent Tests**: All passing including new state flow and timeout tests
- ✅ **Database Cleanup**: Test isolation with proper cleanup before each run
- ✅ **Selective Debug Logging**: DEBUG for app code, INFO for external libraries
- ✅ **Integration Tests**: Real MCP server tests with database cleanup

**Key Progress - What's NOW Implemented:**
- ✅ **Complete Query System**: Users can ask natural language questions about transactions
- ✅ **Intelligent Formatting**: Results automatically formatted based on query type
- ✅ **Unified Storage**: Single source of truth for all structured data
- ✅ **Production-Ready**: Comprehensive error handling, timeouts, and retry logic

**Key Limitations - What's Still NOT Implemented:**
- ❌ **No Advanced Analysis**: No spending patterns, cash flow analysis, or behavioral insights algorithms
- ❌ **No API Integration**: Not integrated with `/conversation/send` endpoint (still uses dummy agent)
- ❌ **No Optimization Engine**: No cost reduction, subscription analysis, or surplus allocation
- ❌ **No Budget Planning**: No conversational budget guidance or recommendations

**Status**: This completes the **transaction storage and query foundation** for the SpendingAgent. Users can now fetch transactions, have them AI-categorized, stored in SQLite, and queried using natural language. The system provides intelligent, context-aware responses with proper formatting based on query type.

#### ✅ COMPLETED: Graphiti Search Enhancement & Code Quality (October 2025)

**What Was Additionally Implemented:**

**Graphiti Integration Enhancements:**
- ✅ **Tag-Based Search Matching**: Added `normalized_month` field to insights for consistent tag matching between storage and search
- ✅ **build_tags() Reuse**: Implemented tag-based Graphiti search using shared `build_tags()` function from InsightsGenerator
- ✅ **Lazy Initialization**: Graphiti client initialized on first use in SpendingAgent following OnboardingAgent singleton pattern
- ✅ **Enhanced Debug Logging**: Added comprehensive debug logging for system prompts and Graphiti search results

**Test Infrastructure:**
- ✅ **Neo4j Cleanup**: Direct database cleanup using Neo4j bolt driver with Cypher queries for test isolation
- ✅ **Enhanced Integration Tests**: Updated `test_spending_agent_real_mcp.py` with Graphiti verification and Neo4j cleanup
- ✅ **Localhost Configuration**: Neo4j cleanup defaults to `localhost:7687` for local development

**Code Quality:**
- ✅ **Ruff Linting**: Auto-fixed 17 linting issues (unused f-strings)
- ✅ **No Duplicate Code**: Verified clean codebase with proper utility function reuse
- ✅ **Dependency Updates**: Added `neo4j>=5.0.0` and `ruff>=0.13.3` as dev dependencies

**Technical Architecture Enhanced:**
```
SpendingAgent → generate_spending_insights → InsightsGenerator.build_tags()
                                           ↓
                                    [normalized_month preserved]
                                           ↓
                              Graphiti Search (same tags) → Historical Context
```

**Key Progress - What's NOW Implemented:**
- ✅ **Consistent Tag Matching**: Search uses exact same normalized month value as storage
- ✅ **Graphiti Context Retrieval**: Historical spending insights retrieved for context-aware responses
- ✅ **Production Logging**: Debug logs for system prompts enable better debugging
- ✅ **Test Cleanup**: Neo4j database cleanup ensures clean test runs

**Testing Coverage:**
- ✅ **All 419 Tests Passing**: Complete test suite validation
- ✅ **Neo4j Driver Integration**: Bolt driver cleanup with graceful fallback
- ✅ **Graphiti Search Verification**: Integration tests validate tag-based search

**Key Limitations - What's Still NOT Implemented:**
- ❌ **No Advanced Analysis**: No spending patterns, cash flow analysis, or behavioral insights algorithms
- ❌ **No Optimization Engine**: No cost reduction, subscription analysis, or surplus allocation
- ❌ **No Budget Planning**: No conversational budget guidance or recommendations

**Status**: This completes the **Graphiti search enhancement and code quality improvements**. Tag-based search now correctly matches stored insights using consistent month normalization, with comprehensive debug logging and clean test infrastructure.

### File List

#### Core Implementation Files

- `backend/app/ai/spending_agent.py` - SpendingAgent LangGraph subgraph with LLM-powered responses, Graphiti search integration, debug logging
- `backend/app/services/transaction_categorization.py` - AI-powered transaction categorization service with structured LLM output
- `backend/app/services/insights_generator.py` - Spending insights generation with normalized_month field and build_tags() utility
- `backend/app/utils/context_formatting.py` - Shared context formatting with intelligent result formatting for transactions, summaries, and breakdowns
- `backend/app/utils/transaction_query_parser.py` - LLM-driven natural language to structured query intent extraction
- `backend/app/utils/transaction_query_executor.py` - Safe parameterized SQL execution engine for transaction queries
- `backend/app/core/database.py` - Unified SQLiteUserStorage for PersonalContext and Transactions
- `backend/app/ai/mcp_clients/graphiti_client.py` - Graphiti MCP client singleton with search and storage capabilities

#### Model & Type Files

- `backend/app/models/plaid_models.py` - Transaction and categorization Pydantic models with AI enhancement fields
- `backend/app/models/transaction_query_models.py` - Query intent models (TransactionQueryIntent, QueryResult, QueryIntent enum)
- `backend/app/services/llm_service.py` - Multi-provider LLM service (OpenAI, Anthropic, Google) with factory pattern
- `backend/app/core/sqlmodel_models.py` - SQLModel definitions for PersonalContext and Transaction tables

#### Test Files

- `tests/test_spending_agent.py` - SpendingAgent tests including query system and state flows (42 tests)
- `tests/test_spending_agent_real_llm.py` - Real LLM integration tests including categorization (20 tests)
- `tests/test_spending_agent_real_mcp.py` - Real MCP server integration tests with database cleanup
- `tests/test_transaction_categorization_service.py` - Transaction categorization service tests (10 tests)
- `tests/test_transaction_categorization_llm.py` - LLM-specific categorization tests (7 tests)
- `tests/test_transaction_storage.py` - Transaction storage and query tests
- `tests/test_user_models.py` - User context and transaction model tests
- `tests/fixtures/transaction_fixtures.py` - Shared transaction test fixtures
- `tests/conftest.py` - Enhanced with shared fixture imports and database cleanup

## QA Results

*This section will be populated by the QA Agent after story completion*
